{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace9c17-2220-477b-81e9-df3dccefbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2 \n",
    "import json\n",
    "import configparser\n",
    "import os\n",
    "from io import StringIO\n",
    "import time\n",
    "import redshift_connector\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba354e-77f5-4fa5-a2b3-3aeafeeff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/toluwalopebabington/projects/DE/covid_data/config_file.config'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c180bb-cd6d-49d3-a4ac-ce74813c271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = config.get('AWS','key')\n",
    "secret = config.get('AWS','secret')\n",
    "region = config.get('AWS','region')\n",
    "DB_name = config.get('DB','DB_name')\n",
    "bucket_name = config.get('s3','Bucket_name')\n",
    "folder_path = config.get('s3','folder_path')\n",
    "staging_area_path  = config.get('s3','staging_area_path')\n",
    "staging_area_folder  = config.get('s3','staging_area_folder')\n",
    "iam_role = config.get('s3','iam_role')\n",
    "DB_user = config.get('DWH','DB_user')\n",
    "DB_Password = config.get('DWH','Password')\n",
    "Workgroup = config.get('DWH','Workgroup')\n",
    "RedShift_DB_name = config.get('DWH','DB_name')\n",
    "DWH_region = config.get('DWH','DWH_region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396d39a-8047-47b6-bbcd-9d4b768c728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client ('s3',region_name = region,\n",
    "                                      aws_access_key_id = key, aws_secret_access_key = secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e993e9cb-7bb6-4710-8d39-cbf47908ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client ('athena',region_name = region,\n",
    "                                      aws_access_key_id = key, aws_secret_access_key = secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820dc56-16f1-4a3a-bc27-e96466820239",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_serverless = boto3.client ('redshift-serverless',region_name = DWH_region,\n",
    "                                      aws_access_key_id = key, aws_secret_access_key = secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ede117-8dff-4c48-a027-be094e3462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = redshift_serverless.get_workgroup(workgroupName = Workgroup)['workgroup']['endpoint']['address']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c552fe4-2550-4547-a1be-1fea6bf0e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create a new bucket to house the raw files then upload the only the csv files in the specified local folder to the s3 bucket\n",
    "'''\n",
    "\n",
    "def create_bucket(bucketname):\n",
    "    try:\n",
    "        s3.create_bucket (Bucket=bucketname, CreateBucketConfiguration={'LocationConstraint'\n",
    "        :region})\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else :\n",
    "        print (bucketname, \" bucket successfully created\")\n",
    "\n",
    "\n",
    "def upload_folder ():\n",
    "    try: \n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    local_path = os.path.join(root,file)\n",
    "                    relative_path = os.path.relpath(local_path, folder_path)\n",
    "                    key = os.path.join(bucket_name, relative_path)\n",
    "                    s3.upload_file(local_path, bucket_name, key)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        print('completed upload to ',bucket_name)\n",
    "\n",
    "create_bucket(bucket_name)\n",
    "upload_folder ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170dca7-727e-4737-94e1-110dce2431fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download_and_load_query_results_upd(client:boto3.client,table) -> pd.DataFrame:\n",
    "\n",
    "    #query the data in the csv files on s3 using Athena\n",
    "    query = \"SELECT * FROM \" + table\n",
    "    query_response = client.start_query_execution(QueryString = query ,\n",
    "    QueryExecutionContext={'Database':DB_name},ResultConfiguration= {\n",
    "    'OutputLocation': staging_area_path ,'EncryptionConfiguration': {'EncryptionOption': 'SSE_S3'}\n",
    "    })\n",
    "\n",
    "    #check if query execution was successful\n",
    "    while True:\n",
    "        execution_response = client.get_query_execution(QueryExecutionId= query_response[\"QueryExecutionId\"])\n",
    "\n",
    "        if execution_response[\"QueryExecution\"][\"Status\"][\"State\"] == \"SUCCEEDED\":\n",
    "            # get query results\n",
    "            while True:\n",
    "                try:\n",
    "                    client.get_query_results(QueryExecutionId=query_response[\"QueryExecutionId\"])\n",
    "                    break\n",
    "                except Exception as err:\n",
    "                    if \"not yet finished\" in str(err):\n",
    "                        time.sleep(0.001)\n",
    "                    else:\n",
    "                        raise err\n",
    "            break\n",
    "        elif execution_response[\"QueryExecution\"][\"Status\"][\"State\"] in ('FAILED','CANCELLED'):\n",
    "            print (\"query execution for \"+ query_response[\"QueryExecutionId\"] +\" \"+ execution_response[\"QueryExecution\"][\"Status\"][\"State\"])\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    \n",
    "    #download the query results and convert to a dataframe    \n",
    "    temp_file_location = \"athena_query_results.csv\"\n",
    "    s3.download_file(bucket_name,f\"{staging_area_folder}/{query_response['QueryExecutionId']}.csv\",temp_file_location)\n",
    "    \n",
    "    return pd.read_csv(temp_file_location)\n",
    "\n",
    "stateabv = Download_and_load_query_results_upd(athena,'state_abv')\n",
    "uscounty = Download_and_load_query_results_upd(athena,'us_county')\n",
    "hospitalbed = Download_and_load_query_results_upd(athena,'hospitalbed')\n",
    "statesdaily = Download_and_load_query_results_upd(athena,'states_daily')\n",
    "usstates = Download_and_load_query_results_upd(athena,'us_States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f48f4-36bd-4be2-9175-26dd0c00c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format all dates the same way\n",
    "statesdaily['date'] = pd.to_datetime(statesdaily['date'],format='%Y%m%d')\n",
    "uscounty['date'] = pd.to_datetime(uscounty['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc68a27-95a8-41ab-b940-8daf109764ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimHospital = hospitalbed[['objectid','hospital_name','hospital_type','state_name','county_name','hq_address','hq_city','hq_state','hq_zip_code','latitude','longtitude']]\n",
    "DimHospital.rename(columns={'objectid':'hospital_id'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b429e1d-e7da-45b7-ba43-165fef81cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates to get distinct state/county names and corresponding fips\n",
    "DimRegion1 = usstates[['state','fips']].drop_duplicates()\n",
    "DimRegion2 = uscounty[['state','fips','county']].drop_duplicates()\n",
    "# merge the dataframes on state column and rename the fips accordingly\n",
    "DimRegion = DimRegion1.merge(DimRegion2, left_on ='state',right_on ='state')\n",
    "DimRegion.rename(columns={'fips_x':'statefips','fips_y':'countyfips'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64794eaf-b0fd-439f-9baa-20c82bd04274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all possible dates from the state and county dataframes\n",
    "DimDate1 = statesdaily[['date']]\n",
    "DimDate2 = uscounty[['date']]\n",
    "# create a union of all the dates, remove duplicates and reset the index\n",
    "DimDate = pd.concat([DimDate1,DimDate2], ignore_index = True).drop_duplicates().reset_index(drop = True)\n",
    "DimDate['Year'] = DimDate['date'].dt.year\n",
    "DimDate['Month'] = DimDate['date'].dt.month\n",
    "DimDate['Quarter'] = DimDate['date'].dt.quarter\n",
    "DimDate.rename(columns={'date':'event_date'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9cda6-73d5-4fe3-8936-58f1e8b885fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fact tables for the state and county levels\n",
    "FactCovidState = statesdaily[['date','state','positive','negative','death','deathconfirmed','deathprobable','hospitalizedcurrently','hospitalizedcumulative','hospitalizeddischarged','recovered']]\n",
    "FactCovidCounty = uscounty[['date','county','state','cases','deaths']]\n",
    "\n",
    "# create a df with hospital_id and the necessary columns needed to merge on the fact tables\n",
    "HospitalId = DimHospital[['hospital_id','state_name','county_name']].merge(stateabv, left_on = 'state_name', right_on = 'state')\n",
    "\n",
    "# Add hospital_id to county fact table\n",
    "FactCovidCounty = FactCovidCounty.merge(HospitalId, left_on ='state',right_on ='state_name').drop(columns = ['state_name','county_name','state_y','abbreviation'])\n",
    "FactCovidCounty.rename(columns={'date':'event_date'},inplace = True)\n",
    "\n",
    "# Add hospital_id to state fact table\n",
    "FactCovidState = FactCovidState.merge(HospitalId, left_on ='state',right_on ='abbreviation').drop(columns = ['county_name','state_y','abbreviation','state_x'])\n",
    "FactCovidState.rename(columns={'date':'event_date'},inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a731b-b57d-410d-b0af-428f961b8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframes to S3\n",
    "def savefiletos3(df,key):\n",
    "    try:\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer,index = False)\n",
    "        s3.put_object(Bucket = bucket_name, Body = csv_buffer.getvalue(), Key = f\"final-tables/{key}.csv\" )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "savefiletos3(FactCovidState,'FactCovidState')\n",
    "savefiletos3(FactCovidCounty,'FactCovidCounty')\n",
    "savefiletos3(DimDate,'DimDate')\n",
    "savefiletos3(DimRegion,'DimRegion')\n",
    "savefiletos3(DimHospital,'DimHospital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf88d65-d207-46f2-9167-513b09f305a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''connect to redshift, create database and tables then load the data into the tables'''\n",
    "\n",
    "\n",
    "def connect_db (dbname):\n",
    "    try:\n",
    "        conn = redshift_connector.connect(host=hostname,database=dbname,user=DB_user,password=DB_Password)\n",
    "        conn.autocommit=True\n",
    "    except redshift_connector.Error as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "        except redshift_connector.Error as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            return conn,cur\n",
    "\n",
    "def run_query (cursor_name,sql_statement):\n",
    "    try:\n",
    "        cursor_name.execute(sql_statement)\n",
    "    except redshift_connector.Error as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        msg = 'success'\n",
    "        return msg\n",
    "        \n",
    "def close_connection (cursor_name,connection):\n",
    "    try:\n",
    "        cursor_name.close()\n",
    "        connection.close()\n",
    "    except redshift_connector.Error as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print (\"database connection successfully closed\")\n",
    "\n",
    "def create_table (tables):\n",
    "    DDL = []\n",
    "    for table in table_list:\n",
    "        query = pd.io.sql.get_schema(table[0],table[1])\n",
    "        DDL.append(query)\n",
    "    return DDL\n",
    "\n",
    "def populate_table (cursor_name,table_name,delimiter):\n",
    "    s3uri = f\"s3://{bucket_name}/final-tables/{table_name}.csv\"\n",
    "    query = f'''copy {table_name} \n",
    "    from '{s3uri}' \n",
    "    iam_role '{iam_role}'\n",
    "    delimiter '{delimiter}' \n",
    "    timeformat 'auto' \n",
    "    region '{region}' \n",
    "    ignoreheader 1'''\n",
    "    \n",
    "    try:\n",
    "        cursor_name.execute(query)\n",
    "    except redshift_connector.Error as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        msg = 'success'\n",
    "        return msg\n",
    "\n",
    "'''\n",
    "connect to base database and create a new database for the dataset\n",
    "closeout from the database connection when the action is completed\n",
    "successfully\n",
    "'''\n",
    "conn,cur = connect_db(RedShift_DB_name)\n",
    "\n",
    "if conn and cur is not None:\n",
    "    response = run_query(cur,\"CREATE DATABASE coviddata;\")\n",
    "    if response == 'success':\n",
    "        close_connection(conn,cur) \n",
    "\n",
    "'''\n",
    "connect to the coviddata database and create the tables\n",
    "'''\n",
    "conn2,cur2 = connect_db('coviddata')\n",
    "\n",
    "if conn2 and cur2 is not None:\n",
    "    table_list =[(FactCovidState,'FactCovidState'),(FactCovidCounty,'FactCovidCounty'),\n",
    "    (DimDate,'DimDate'),(DimRegion,'DimRegion'),(DimHospital,'DimHospital')]\n",
    "\n",
    "    Queries = create_table(table_list)\n",
    "\n",
    "    for query in Queries:\n",
    "        response2 = run_query(cur2,query)\n",
    "\n",
    "#if all the tables were created successfully, populate the tables\n",
    "    if  response2 =='success':\n",
    "        for table in table_list:\n",
    "            response3 = populate_table(cur2,table[1],\",\")\n",
    "        if response3 == 'success':\n",
    "            close_connection(conn2,cur2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0ee90-93b9-4601-acfb-76aae4fb94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glue script to copy the data from S3 to redshift\n",
    "\n",
    "import configparser\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3 = boto3.client ('s3')\n",
    "file1,file2 = \"configfile.config\",\"create_statements.txt\"\n",
    "s3.download_file(\"covid-tolub\",\"config_file.config\",file1)\n",
    "s3.download_file(\"covid-tolub\",\"create_statements.txt\",file2)\n",
    "    \n",
    "config = configparser.ConfigParser()\n",
    "config.read(file1)\n",
    "\n",
    "region = config.get('AWS','region')\n",
    "bucket_name = config.get('s3','Bucket_name')\n",
    "iam_role = config.get('s3','iam_role')\n",
    "DB_user = config.get('DWH','DB_user')\n",
    "DB_Password = config.get('DWH','Password')\n",
    "RedShift_DB_name = config.get('DWH','DB_name')\n",
    "hostname =  config.get('DWH','host')\n",
    "\n",
    "with open('create_statements.txt', 'r') as file:\n",
    "    queries = file.readlines()\n",
    "    queries = [query.strip() for query in queries]\n",
    "    \n",
    "table_list =['FactCovidState','FactCovidCounty','DimDate','DimRegion','DimHospital']\n",
    "    \n",
    "\n",
    "def connect_db (dbname):\n",
    "    try:\n",
    "        conn = redshift_connector.connect(host=hostname,database=dbname,user=DB_user,password=DB_Password)\n",
    "        conn.autocommit=True\n",
    "    except redshift_connector.Error as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "        except redshift_connector.Error as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            return conn,cur\n",
    "\n",
    "def run_query (cursor_name,sql_statement):\n",
    "    try:\n",
    "        cursor_name.execute(sql_statement)\n",
    "    except redshift_connector.Error as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        msg = 'success'\n",
    "        return msg\n",
    "        \n",
    "def close_connection (cursor_name,connection):\n",
    "    try:\n",
    "        cursor_name.close()\n",
    "        connection.close()\n",
    "    except redshift_connector.Error as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print (\"database connection successfully closed\")\n",
    "\n",
    "def populate_table (cursor_name,table_name,delimiter):\n",
    "    s3uri = f\"s3://{bucket_name}/final-tables/{table_name}.csv\"\n",
    "    query = f'''copy {table_name} \n",
    "    from '{s3uri}' \n",
    "    iam_role '{iam_role}'\n",
    "    delimiter '{delimiter}' \n",
    "    timeformat 'auto' \n",
    "    region '{region}' \n",
    "    ignoreheader 1'''\n",
    "    \n",
    "    try:\n",
    "        cursor_name.execute(query)\n",
    "    except redshift_connector.Error as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        msg = 'success'\n",
    "        return msg\n",
    "        \n",
    "        \n",
    "'''\n",
    "connect to base database and create a new database for the dataset\n",
    "closeout from the database connection when the action is completed\n",
    "successfully\n",
    "'''\n",
    "conn,cur = connect_db(RedShift_DB_name)\n",
    "\n",
    "if conn and cur is not None:\n",
    "    response = run_query(cur,\"CREATE DATABASE coviddata;\")\n",
    "    if response == 'success':\n",
    "        close_connection(conn,cur) \n",
    "\n",
    "'''\n",
    "connect to the coviddata database and create the tables\n",
    "'''\n",
    "conn2,cur2 = connect_db('coviddata')\n",
    "\n",
    "if conn2 and cur2 is not None:\n",
    "   \n",
    "    for query in queries:\n",
    "        response2 = run_query(cur2,query)\n",
    "\n",
    "#if all the tables were created successfully, populate the tables\n",
    "    if  response2 =='success':\n",
    "        for table in table_list:\n",
    "            response3 = populate_table(cur2,table[1],\",\")\n",
    "        if response3 == 'success':\n",
    "            close_connection(conn2,cur2) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
